---
layout: post
title:  "Recent Approaches on Accelerating Transformer-based Deep Neural Network"
date:   2023-08-27 00:00:00+0900
blurb: "A look at an example post using Bay Jekyll theme."
og_image: /assets/img/content/post-example/Banner.jpg
---


<!-- <img src="{{ "/assets/img/content/post-example/Banner.jpg" | absolute_url }}" alt="bay" class="post-pic"/> -->




### CONTENTS

<span style="font-size: 18px;">
&ensp; &ensp; [1 &nbsp; &nbsp; Introduction](#1-introduction)
<br>
&ensp; &ensp; [2 &nbsp; &nbsp; Background](#2-background)
<br>
&ensp; &ensp; [3 &nbsp; &nbsp; Acceleration Techniques](#3-acceleration-techniques)
<br>
&ensp; &ensp; [4 &nbsp; &nbsp; Discussion](#4-discussion)
<br>
&ensp; &ensp; [5 &nbsp; &nbsp; Conclusion](#5-conclusion)
<br>
&ensp; &ensp; [6 &nbsp; &nbsp; Reference](#6-reference)
<br>
</span>


### 1 INTRODUCTION

<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; <span style="font-size: 26px;">R</span>ecently, transformer-based deep neural network outperforms in various AI domains such as
NLP (natural language processing), and CV (computer vision). Its key mechanism is <em> attention </em> which was first introduced in [[1]](#6-reference).
Attention mechanism allows model to attend to tokens differently according to their contextual importance. However, attention mechanism
requires high cost of computation in that its complexity is quadratically proportional to input token length. This limits maximum input sequence length
of model (such as 1024 or 2048), because inference latency and power consumption are important constraints in real world applications using deep neural network.
</span>



<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; To address this fundamental limitation, there have been many researches on accelerating transformer especially focused on attention. Two main
algorithmic optimization schemes are <em> quantization </em> and <em> pruning </em>. With these optimization techniques, more hardware efficient implementation is
possible. Compared to execution of inference in general purpose processor such as CPU or GPU, offloading some part of compuation (or maybe full) to dedicated hardware can
achieve high speedup and energy efficiency. It may contribute to more efficient execution of deep neural network applications, and expand limitation of model
capacity such as maximum token length potentially allowing model to do some more complicated and challenging task.
</span>

<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; This article will introduce some backgrounds on attention mechanism and mainly analyze two main accelerating techniques, quantization
and pruning with several recent researches. Finally, I will discuss about this trends and further research topics.
</span>

### 2 BACKGROUND

<span style="font-size: 20px;">
2.1 Transformer Layer and Attention Mechanism
</span>

<img src="{{ "/assets/img/content/transformer/attention_operation.png" | absolute_url }}" alt="attention" >
<br>

<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; Transformer-based neural network consists of a stack of transformer layers and additional epilogue layer depending on its target task.
Number of transformer layer varies according to model capacity. For example, BERT-base uses 12 layers whereas BERT-large uses 24 [[2]](#6-reference). 
<br>
&nbsp; &nbsp; &nbsp; A single transformer layer is shown in left part of Figure 1. It is composed of three main parts:
<strong> <em> QKV generation, Multi-head attention, Feed forward network (FFN). </em> </strong>
 Dimension of block input is (n, d<sub>model</sub>) where n is sequence length and d<sub>model</sub> is embedding
dimension (768 for BERT-base) [[2]](#6-reference). In QKV generation stage, the block input is processed by three different fully connected layers,
resulting in three matrices: Q (Query), K (Key), and V (Value). QKV matrices are divided into h heads and processed by multi-head attention layer. Let's take
a look at the operation of single head attention layer, shown in the right part of Figure 1. Matrix multiplication of Q and transpose of K is processed by
row-wise softmax operation. This result is especially referred to as the <em> attention score(=probability) </em> which indicates the degree of correlation
between each query and key. Finally, attention output is generated by weighted sum of row of value matrix whose weight is attention score of corresponding
query. Each head generates an attention output of shape (n, d<sub>k</sub>) and all the results are concatenated, resulting in its shape being
(n, d<sub>model</sub>). Usually this concatenated output pass through additional fc layer, generating same shape.
Feed forward network is consist of two fc layers and hidden dimension of fc layer
is usually larger than d<sub>model</sub>.
For example, hidden dimension of FFN layer is 3,072 which is 4 times of d<sub>model</sub> in BERT-base model [[2]](#6-reference).
</span>


<span style="font-size: 20px;">
2.2 Computational Complexity
</span>

<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; This section analyze the computational complexity of 3 main operations in a transformer layer. It provides some motivation in accelerating
the operations. As these 3 operations fundamentally involve matrix multiplication, analyzing dimensions of operand matrices is required.
<br>
&nbsp; &nbsp; &nbsp; Firstly, in the QKV generation stage, an input matrix of dimension (n, d<sub>model</sub>) is multiplied by 
a matrix of dimension (d<sub>model</sub>, d<sub>model</sub>)
, resulting in a computational complexity of of O(n &#183; d<sub>model</sub><sup>2</sup>).
Secondly, in the attention layer, the computational complexity for both multiplication of Q &#183; K<sup>T</sup> and attention score with V
is O(n<sup>2</sup> &#183; d<sub>model</sub>). 
Thirdly, in the FFN layer, an input matrix of dimension (n, d<sub>model</sub>) is multiplied by 
a matrix of dimension (d<sub>model</sub>, c &#183; d<sub>model</sub>) where c is a constant. Complexity is O(n &#183; d<sub>model</sub><sup>2</sup>) which
is same as in the QKV generation.
</span>

### 3 ACCELERATION TECHNIQUES
<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; Three main approaches in accelerating neural network are dataflow, sparsity, and quantization [[3]](#6-reference). As dataflow is
fundamentally related to all the HW-based acceleration schemes, this article will primarily focus on the other two:
<strong> <em> sparsity/pruning and quantization </em> </strong>.
</span>

<span style="font-size: 20px;">
3.1 Sparsity/Pruning
</span>

<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; Sparsity enables more efficient HW-based matrix multiplication by effectively skipping multiplications with zero values. Pruning aims to
convert unimportant values, such as thosee near-zero value, into zeros, enhancing the sparsity of the matrix.
</span>

<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; Main approaches of pruning is to select only important (highly correlated) keys for each query, thereby increasing the sparsity of
attention score matrix ([[4], [5], [6]](#6-reference)). To identify a set of important keys, prior research has suggested methods for the
approximate computation of attention matrix (before softmax).
<br>
&nbsp; &nbsp; &nbsp;  A<sup>3</sup> [[4]](#6-reference) approximately computes q &#183; K<sup>T</sup>,
where q is a single query vector, by iteratively computing the largest and smallest elementwise multiplications. This process can be done
efficiently by sorting all feature vectors of the key matrix of length n by their values. Keys corresponding to non-zero values in attention vector are
classified as candidates. Full attention scores are computed only for keys belonging to these candidates, effectively reducing the number of elementwise
multiplications. After the softmax operation on the attention matrix, further pruning is performed by selecting only elements that exceed 
T% of the largest element in each row. This pruning can reduce the number of multiplications in the weighted sum of the value matrix.
<br>
&nbsp; &nbsp; &nbsp; ELSA [[5]](#6-reference) also approximately computes q &#183; K<sup>T</sup>,
by estimating the angular distance between q and k with binary hashing. d-dimensional query vector q and key vector k are mapped into k-dimensional binary
hash vectors using signed projection with k d<sub>model</sub>-dimensional orthogonal vectors:
<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
h(x) = (h<sub>v1</sub>(x), ... , h<sub>vk</sub>(x)) where h<sub>v</sub>(x) = sign(v &#183; x), and v &isin; <strong>R</strong><sup>d</sup>
<br>
It estimates the angular distance between q and k from their Hamming distance since they become binary after hashing. Finally, the similarity, q &#183;
k<sup>T</sup>, is obtained by applying the cosine function to the estimated angular distance and multiplying it by the norm of the key vector.
Important keys are selected by comparing them with layer-specific threshold values,
which are determined offline by running several inferences on the training set.
<br>
&nbsp; &nbsp; &nbsp; FACT [[6]](#6-reference) extends approximate computation to QK generation. It uses a leading-one detector as an approximation of the INT
number which has the effect of rounding down to the nearest power of two. With two operands that are powers of two, costly multiplication can be replaced with
a low-power shift-and-add operation. With this approximate computation on QK generation and attention, it obtains the top-k keys for each query. Based on these
top-k keys, it skips generating rows of K and V that are not selected by the approximate attention matrix. In matrix Q, the row in which the maximum value
dominates the other elements within the same row is replaced with a one-hot vector (only the maximum element is set to 1.0 while others are all zero)
thereby enabling the skipping of Q generation for several rows.
</span>

<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; Spatten [[7]](#6-reference) proposes cascade pruning based on the intuition that unimportant tokens and heads can be removed 
safely with little impact on the final results. As the number of removed tokens and heads increases while processing the transformer layers, it can 
continuously reduce the number of computation in a single layer during inference, whereas pruning in previous works [[4], [5], [6]](#6-reference) was independent
between each layer. To determine token importance, the attention probabilities for each key are accumulated over all the queries and heads. Similarily, 
head importance is determined by accumulating all the absolute values of the attention output for each head. Top-k information for token and head is used in
the QKV generation of the next transformer layer, generating QKV only for selected tokens and heads.
</span>



<span style="font-size: 20px;">
3.2 Quantization
</span>

<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; quantization here
<span>


### 4 DISCUSSION

<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; My discussion here
</span>

### 5 CONCLUSION
<span style="font-size: 16px;">
&nbsp; &nbsp; &nbsp; My conclusion here
</span>

### 6 REFERENCE
<span style="font-size: 16px;">
[1] &nbsp; A. Vaswani. "Attention is all you need." <em> Advances in neural information processing systems 30 </em> (2017).
<br>
[2] &nbsp; J. Devlin. "Bert: Pre-training of deep bidirectional transformers for language understanding." <em> arXiv preprint arXiv:1810.04805 </em> (2018).
<br>
[3] &nbsp; J.W. Jang. "Sparsity-aware and re-configurable NPU architecture for Samsung flagship mobile SoC."
<em> 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA). </em> IEEE, 2021.
<br>
[4] &nbsp; T.J. Ham. "A<sup>3</sup>: Accelerating attention mechanisms in neural networks with approximation." 
<em> 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). </em> IEEE, 2020.
<br>
[5] &nbsp; T.J. Ham, Y.J. Lee. "ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks."
<em> 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA). </em> IEEE, 2021.
<br>
[6] &nbsp; Y. Qin. "FACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction."
<em> Proceedings of the 50th Annual International Symposium on Computer Architecture. </em> 2023.
<br>
[7] &nbsp; H. Wang. "Spatten: Efficient sparse attention architecture with cascade token and head pruning."
<em> 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). </em> IEEE, 2021.
</span>


